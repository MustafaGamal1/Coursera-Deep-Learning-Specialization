## Week 3 Quiz

Question 1. Which of the following are true? (Check all that apply.)

- [x] X is a matrix in which each column is one training example.
- [x] a^{[2](12)} denotes the activation vector of the 2^{nd} layer for the 12^{th} training example.
- [x] a^{[2]}_4 is the activation output by the 4^{th} neuron of the 2^{nd} layer
- [ ] a^{[2]}_4 is the activation output of the 2^{nd} layer for the 4^{th} training example
- [ ] a^{[2](12)} denotes activation vector of the 12^{th}layer on the 2^{nd}training example.
- [x] a^{[2]} denotes the activation vector of the 2^{nd}layer.

Question 2. The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. True/False?

- [x] True
- [ ] False

Note: Yes, As seen in lecture the output of the tanh is between -1 and 1, it thus centers the data which makes the learning simpler for the next layer.

 Question 3. Which of these is a correct vectorized implementation of forward propagation for layer l, where 1<= l <= L?
 
 
 
